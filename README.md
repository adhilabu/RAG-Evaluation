# Document Processing System

A production-grade system for processing large documents (500+ pages) with parallel pipelines for **Map-Reduce Summarization** and **RAG Storage** using LangGraph, FastAPI, and Streamlit.

## ğŸ—ï¸ Architecture

### Dual Pipeline System

1. **Summarization Pipeline** (LangGraph Map-Reduce)
   - Extract and clean PDF text
   - Split into large chunks (10k-20k tokens)
   - **Map**: Parallel LLM summarization of each chunk
   - **Reduce**: Synthesize final cohesive summary

2. **Storage Pipeline** (RAG with Qdrant)
   - Split into small chunks (512-1024 tokens)
   - Generate embeddings (OpenAI text-embedding-3-small)
   - Store in Qdrant vector database
   - Enable semantic search

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Docker (for Qdrant)
- OpenAI API key

### 1. Start Qdrant (if not running)

```bash
docker run -p 6333:6333 -p 6334:6334 \
    -v $(pwd)/qdrant_storage:/qdrant/storage:z \
    qdrant/qdrant:v1.12.0
```

### 2. Install Dependencies

```bash
cd /Users/adhilabubacker/Projects/ai-projects/RAG
pip install -r requirements.txt
```

### 3. Configure Environment

```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
```

### 4. Start Backend API

```bash
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

API will be available at: http://localhost:8000
API docs: http://localhost:8000/docs

### 5. Start Streamlit UI

```bash
# In a new terminal
cd frontend
streamlit run streamlit_app.py
```

UI will be available at: http://localhost:8501

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ backend/               # FastAPI application
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py       # FastAPI app
â”‚       â”œâ”€â”€ config.py     # Configuration
â”‚       â””â”€â”€ api/          # API endpoints
â”‚           â”œâ”€â”€ upload.py      # Document upload
â”‚           â”œâ”€â”€ summarize.py   # Summarization
â”‚           â””â”€â”€ query.py       # Search
â”œâ”€â”€ document_processor/    # PDF extraction & chunking
â”‚   â”œâ”€â”€ extractor.py      # PyMuPDF extraction
â”‚   â”œâ”€â”€ cleaner.py        # Text cleaning
â”‚   â””â”€â”€ chunker.py        # Dual chunking strategy
â”œâ”€â”€ langgraph_pipeline/    # LangGraph map-reduce
â”‚   â”œâ”€â”€ state.py          # State schema
â”‚   â”œâ”€â”€ nodes.py          # Map/Reduce nodes
â”‚   â””â”€â”€ graph.py          # Workflow definition
â”œâ”€â”€ rag_storage/          # Vector database operations
â”‚   â”œâ”€â”€ qdrant_client.py  # Qdrant manager
â”‚   â”œâ”€â”€ embeddings.py     # Embedding generation
â”‚   â””â”€â”€ retrieval.py      # Search functions
â”œâ”€â”€ evaluation/           # RAG evaluation framework
â”‚   â”œâ”€â”€ evaluation_dataset.py    # Dataset structures
â”‚   â”œâ”€â”€ retrieval_metrics.py     # Retrieval metrics
â”‚   â”œâ”€â”€ generation_metrics.py    # Generation metrics
â”‚   â”œâ”€â”€ evaluation_pipeline.py   # Evaluation orchestrator
â”‚   â””â”€â”€ visualizations.py        # Result visualizations
â”œâ”€â”€ scripts/              # Utility scripts
â”‚   â”œâ”€â”€ run_evaluation.py        # Main evaluation runner
â”‚   â””â”€â”€ create_evaluation_dataset.py  # Dataset creator
â”œâ”€â”€ frontend/             # Streamlit UI
â”‚   â””â”€â”€ streamlit_app.py  # Main UI
â”œâ”€â”€ data/                 # Uploaded PDFs storage
â”œâ”€â”€ evaluation_datasets/  # Evaluation datasets
â””â”€â”€ requirements.txt      # Python dependencies
```

## ğŸ¯ Usage

### 1. Upload a Document

1. Go to **Upload Document** page
2. Select a PDF file
3. Click **Process Document**
4. System will:
   - Extract text from PDF
   - Create small chunks for RAG (stored in Qdrant)
   - Create large chunks for summarization
   - Generate embeddings
   - Return a document ID

### 2. Generate Summary

1. Go to **Summarize** page
2. Select your document
3. Click **Generate Summary**
4. LangGraph will:
   - Distribute chunks to parallel workers
   - Summarize each chunk independently
   - Synthesize all summaries into final summary

### 3. Search Documents

1. Go to **Search Documents** page
2. Enter your query
3. Adjust settings (number of results, similarity threshold)
4. Optionally filter by specific document
5. View results with similarity scores

## ğŸ”§ API Endpoints

### Upload
- `POST /api/v1/upload` - Upload and process PDF
- `GET /api/v1/documents` - List all documents
- `GET /api/v1/documents/{doc_id}` - Get document details

### Summarization
- `POST /api/v1/summarize` - Trigger summarization
- `GET /api/v1/summarize/{doc_id}/status` - Get summary status

### Search
- `POST /api/v1/query` - Semantic search
- `GET /api/v1/collection/info` - Qdrant collection info

## ğŸ§ª Testing

```bash
# Test document processing
python -c "
from document_processor import extract_pdf_text, clean_pages, create_rag_chunks
pages = extract_pdf_text('data/sample.pdf')
cleaned = clean_pages(pages)
chunks = create_rag_chunks(cleaned, document_id='test')
print(f'Created {len(chunks)} chunks')
"

# Test Qdrant connection
python -c "
from rag_storage import QdrantManager
qm = QdrantManager()
print(qm.get_collection_info())
"
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **PDF Extraction** | PyMuPDF | Fast text extraction |
| **Text Processing** | LangChain | Smart text splitting |
| **Orchestration** | LangGraph | Map-reduce workflow |
| **LLM** | OpenAI GPT-4o-mini | Summarization |
| **Embeddings** | OpenAI text-embedding-3-small | Vector generation |
| **Vector DB** | Qdrant | Semantic search |
| **Backend** | FastAPI | REST API |
| **Frontend** | Streamlit | User interface |

## ğŸ“Š Key Features

âœ… **Intelligent Chunking**: Dual strategy for RAG vs Summarization
âœ… **Parallel Processing**: LangGraph async map-reduce
âœ… **Semantic Search**: Vector similarity with Qdrant
âœ… **Metadata Tracking**: Page numbers, chunk indices
âœ… **Production Ready**: Error handling, validation
âœ… **Interactive UI**: Real-time progress tracking

## ğŸ” Environment Variables

Required in `.env`:

```bash
OPENAI_API_KEY=sk-...           # Required
QDRANT_HOST=localhost            # Default
QDRANT_PORT=6333                 # Default
```

## ğŸ“ Notes

- **Maximum File Size**: 50MB (configurable in `config.py`)
- **Embedding Dimensions**: 1536 (text-embedding-3-small)
- **RAG Chunk Size**: 1000 characters (~250 tokens)
- **Summary Chunk Size**: 15000 characters (~3750 tokens)
- **Default LLM**: GPT-4o-mini

## ğŸ› Troubleshooting

### API won't start
- Check if port 8000 is available
- Verify `.env` file exists with OpenAI API key

### Qdrant connection error
- Ensure Qdrant Docker container is running: `docker ps`
- Check Qdrant dashboard: http://localhost:6333/dashboard

### Summarization fails
- Check OpenAI API key is valid
- Verify document has been uploaded first
- Check backend logs for detailed errors

## ğŸ“Š RAG Evaluation

The system includes a comprehensive evaluation framework to measure and analyze RAG performance.

### Evaluation Metrics

#### Retrieval Metrics
- **Precision@K**: Proportion of retrieved documents that are relevant
- **Recall@K**: Proportion of relevant documents that were retrieved
- **MRR (Mean Reciprocal Rank)**: Average reciprocal rank of first relevant result
- **NDCG@K**: Normalized Discounted Cumulative Gain (handles graded relevance)
- **Hit Rate@K**: Whether at least one relevant document appears in top K

#### Generation Metrics (RAGAS)
- **Faithfulness**: Whether generated answer is grounded in retrieved context
- **Answer Relevancy**: How relevant the answer is to the query
- **Context Precision**: How relevant the retrieved contexts are
- **Context Recall**: Whether all relevant information was retrieved

### Creating Evaluation Datasets

Create evaluation datasets with queries and ground truth:

```bash
python scripts/create_evaluation_dataset.py --output evaluation_datasets/my_eval.json
```

Or manually create a JSON file:

```json
{
  "examples": [
    {
      "query": "What is the main topic?",
      "relevant_doc_ids": ["doc1_chunk_0", "doc1_chunk_1"],
      "relevance_scores": {"doc1_chunk_0": 2.0, "doc1_chunk_1": 1.0},
      "ground_truth_answer": "Optional expected answer"
    }
  ]
}
```

### Running Evaluations

**Retrieval-only evaluation**:
```bash
python scripts/run_evaluation.py \
  --dataset evaluation_datasets/sample_eval.json \
  --output evaluation_results/run_001 \
  --k-values 1,3,5,10
```

**End-to-end evaluation** (includes generation quality):
```bash
python scripts/run_evaluation.py \
  --dataset evaluation_datasets/sample_eval.json \
  --output evaluation_results/run_001 \
  --k-values 1,3,5,10 \
  --include-generation
```

### Evaluation Outputs

Results are saved to the specified output directory:
- `results.json` - Complete evaluation results
- `report.md` - Human-readable markdown report
- `metrics_by_k.png` - Visualization of metrics across K values
- `score_distribution.png` - Distribution of scores
- `ragas_metrics.png` - Generation quality metrics (if applicable)
- `results.csv` - Results in CSV format

### Interpreting Results

**Good Retrieval Performance**:
- Precision@5 > 0.6
- Recall@5 > 0.7
- NDCG@5 > 0.7
- MRR > 0.5

**Good Generation Performance**:
- Faithfulness > 0.7
- Answer Relevancy > 0.7
- Context Precision > 0.6



## ğŸš§ Future Enhancements

- [ ] Add Neo4j GraphRAG integration
- [ ] Implement reranking (Cohere/Cross-encoder)
- [ ] Add PostgreSQL checkpointer for LangGraph
- [ ] Support more file formats (DOCX, TXT)
- [ ] Add authentication and user management
- [ ] Deploy with Docker Compose

## ğŸ“„ License

MIT

## ğŸ‘¨â€ğŸ’» Author

Built as a demonstration of production-grade RAG and Map-Reduce summarization systems.
